{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6080ca6",
   "metadata": {},
   "source": [
    "\n",
    "# On‚ÄëPrem Large Language Models with **Ollama**\n",
    "\n",
    "Welcome! This notebook shows how to **run state‚Äëof‚Äëthe‚Äëart language models entirely on your own hardware** using [Ollama](https://ollama.com).\n",
    "\n",
    "**Replace External LLM APIs with Ollama!**\n",
    "\n",
    "Learn how to replace cloud-based LLMs (OpenAI, Anthropic, etc.) with local Ollama models in your AI agents. This notebook covers everything from basic API calls to complete agent migration.\n",
    "\n",
    "Ollama is perfect for getting started with local LLMs, though advanced users may later explore alternatives like vLLM, TensorRT-LLM, or custom inference servers for maximum performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Overview  \n",
    "**Ollama** is a lightweight runtime that lets you download, run, and interact with open‚Äëweight LLMs (like Llama¬†3) through a local REST API. There is **no external cloud** involvement and your data never leaves the machine.\n",
    "\n",
    "| Section | What you‚Äôll learn |\n",
    "|---------|-------------------|\n",
    "| 1. Installation | Set up the Ollama daemon on macOS¬†/ Linux¬†/ Windows¬†or Docker |\n",
    "| 2. Model¬†Download | Pull a quantised model file (`.gguf`) and start the server |\n",
    "| 3. Test Your Environment | Send a chat request with `curl` and inspect the streaming response |\n",
    "| 4. Python¬†API | Call the REST endpoint from `requests` and **LangChain‚Äôs** `ChatOllama` wrapper |\n",
    "| 5. API Parameters | Customize model behavior with parameters like temperature, top_p, and system prompt |\n",
    "| 6. Troubleshooting | Solve common issues |\n",
    "| 7. Model Recommendations | A summary of different open-source LLMs |\n",
    "\n",
    "---\n",
    "\n",
    "## Motivation  \n",
    "Many organisations need generative‚ÄëAI capabilities **without sending sensitive IP to external services**. Running models on‚Äëprem offers:\n",
    "\n",
    "* **Data sovereignty** ‚Äì full control over data and model weights  \n",
    "* **Predictable costs** ‚Äì no per‚Äëtoken fees, just hardware utilisation  \n",
    "* **Low latency** ‚Äì everything happens on¬†LAN speeds  \n",
    "* **Flexibility** ‚Äì swap models, tweak quantisation, or fine‚Äëtune offline\n",
    "\n",
    "---\n",
    "\n",
    "## Key Components  \n",
    "\n",
    "1. **Ollama Daemon** ‚Äì background process that loads the model and exposes a REST/GRPC interface.  \n",
    "2. **Model Files** (`.gguf`) ‚Äì quantised weights optimised for consumer‚Äëgrade GPUs/CPUs.  \n",
    "3. **Client Interfaces** ‚Äì CLI (`ollama run ...`), REST, Python SDK, LangChain integration.\n",
    "\n",
    "---\n",
    "\n",
    "## Method Details  \n",
    "\n",
    "We will walk through:\n",
    "\n",
    "1. **Installation** ‚Äì one‚Äëline script or Docker image.  \n",
    "2. **Model Pull & Serve** ‚Äì grab *Llama¬†3.1 8‚Äëb* and launch the runtime.  \n",
    "3. **First Chat Request** ‚Äì send a `curl` call and watch the JSON stream.  \n",
    "4. **Python Requests Client** ‚Äì wrap the call in pure Python.  \n",
    "5. **LangChain Integration** ‚Äì drop‚Äëin replacement for cloud models.  \n",
    "\n",
    "---\n",
    "\n",
    "## Benefits \n",
    "\n",
    "* ‚úÖ Keeps confidential data on-premises, making it suitable for enterprise environments and sensitive workloads\n",
    "* ‚úÖ Operates fully offline or in air-gapped systems, which is essential for secure or disconnected use cases\n",
    "* ‚úÖ Simple to install and run, with no need for complex setup or reliance on cloud services\n",
    "* ‚úÖ Easily integrates with existing tools, including CLI, REST API, LangChain, and others\n",
    "* ‚úÖ Eliminates per-token charges and API limits, offering a cost-effective solution for large-scale or continuous use\n",
    "* ‚úÖ Gives you full control over models and configurations, allowing customization for performance, size, and behavior\n",
    "\n",
    "> **Is Ollama relevant for you?**  \n",
    "> Ollama is especially useful when you need to run large language models on-premises for privacy, security, regulatory compliance, or to avoid cloud dependencies. Whether you're building internal tools, developing AI agents for sensitive workflows, or operating in air-gapped environments, Ollama delivers a flexible and reliable local LLM solution.\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "> **üìù Note:** All examples in this notebook use `stream: false` for simplicity and easier response handling. You can check out the `stream: true` to receive responses as they're generated in real-time, which is great for interactive applications but requires handling partial responses.\n",
    "\n",
    "### Minimal Hardware Requirements\n",
    "\n",
    "- **RAM**: 8GB minimum (16GB+ recommended for larger models)\n",
    "- **Storage**: 10GB+ free space (models range from 4GB to 70GB+)\n",
    "- **CPU**: Any modern x64 processor (ARM64 also supported on macOS)\n",
    "- **GPU**: Optional but recommended (NVIDIA, AMD, or Apple Silicon for acceleration)\n",
    "\n",
    "---\n",
    "\n",
    "*Run each cell with* <kbd>Shift</kbd>¬†+¬†<kbd>Enter</kbd>, *or use the ‚ÄúRun All‚Äù button in the toolbar.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aec0fc",
   "metadata": {},
   "source": [
    "# 1. Installation\n",
    "Run the following command to download and install the Ollama daemon for macOS‚ÄØ/‚ÄØLinux. Windows users can grab the `.exe` from the official site."
   ]
  },
  {
   "cell_type": "raw",
   "id": "5f2a4ad3-9982-4799-9900-a1eb4b666f31",
   "metadata": {
    "language": "bash"
   },
   "source": [
    "# macOS/Linux\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# Windows: Download .exe file from https://ollama.com/download and install it. Note, Ollama will lunch directly after installing, no need for further action.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1befc3-9528-4155-97a3-76d99e91bf83",
   "metadata": {},
   "source": [
    "> **üí° Tip**: You can also run Ollama using Docker containers! See the [official Docker image guide](https://ollama.com/blog/ollama-is-now-available-as-an-official-docker-image) for setup instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce92896",
   "metadata": {},
   "source": [
    "# 2. Download a pre‚Äëtrained model & start the server\n",
    "`ollama pull` fetches the quantised weights. `ollama serve` launches the local REST service."
   ]
  },
  {
   "cell_type": "raw",
   "id": "04e597e7-7b02-4b2e-8f96-d1543589f7e4",
   "metadata": {
    "language": "bash"
   },
   "source": [
    "ollama pull llama3.1:8b  # For all avilable models visit: https://ollama.com/library\n",
    "ollama serve             # Start the service\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebcee41",
   "metadata": {},
   "source": [
    "\n",
    "> **‚ö†Ô∏è Windows Note**: If you get \"only one usage of each socket address is permitted\", Ollama is likely already running. On Windows, Ollama typically starts automatically after installation. Try skipping `ollama serve` and go directly to testing.\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e88eaec",
   "metadata": {},
   "source": [
    "# 3. Test Your Environmen\n",
    "Make your first chat request via REST API\n",
    "\n",
    "This `curl` call sends a simple message to the local server and streams the response back as JSON.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca89785-51fd-4374-bcfc-a1543139464d",
   "metadata": {},
   "source": [
    "\n",
    "**Linux / IOS command:**\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f4dbd376-6cc3-4f30-bd62-5b01c777646a",
   "metadata": {
    "language": "bash"
   },
   "source": [
    "curl http://localhost:11434/api/chat -d '{\n",
    "  \"model\": \"llama3.1:8b\",\n",
    "  \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}],\n",
    "  \"stream\": false\n",
    "}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c865b6",
   "metadata": {},
   "source": [
    "\n",
    "**Windows CMD alternative:**\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b8730eb7-da25-4963-94ee-eff29493c916",
   "metadata": {
    "language": "bash"
   },
   "source": [
    "curl -X POST http://localhost:11434/api/chat  -H \"Content-Type: application/json\"  -d \"{\\\"model\\\": \\\"llama3.1:8b\\\", \\\"stream\\\": false, \\\"messages\\\": [{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Hello!\\\"}]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f660a9",
   "metadata": {},
   "source": [
    "\n",
    "**Postman alternative:**\n",
    "- **Method**: POST\n",
    "- **URL**: `http://localhost:11434/api/chat`\n",
    "- **Headers**: `Content-Type: application/json`\n",
    "- **Body** (raw JSON):\n",
    "{\n",
    "  \"model\": \"llama3.1:8b\",\n",
    "  \"stream\": false,\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\", \n",
    "      \"content\": \"Hello!\"\n",
    "    }\n",
    "  ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b8590c",
   "metadata": {},
   "source": [
    "\n",
    "# 4. Python API\n",
    "\n",
    "### Replace OpenAI API Calls\n",
    "\n",
    "**Before (OpenAI):**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7430dde2",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "prompt = \"Hello!\"\n",
    "client = OpenAI(api_key=\"your-key\")\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e2a011",
   "metadata": {},
   "source": [
    "\n",
    "**After (Ollama):**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1d9ba3",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "prompt = \"Hello!\"\n",
    "response = requests.post(\"http://localhost:11434/api/chat\", json={\n",
    "    \"model\": \"llama3.1:8b\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "    \"stream\": False\n",
    "})\n",
    "data = response.json()\n",
    "print(data[\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4c1f7c",
   "metadata": {},
   "source": [
    "\n",
    "### Replace LangChain Models\n",
    "\n",
    "**Before (OpenAI):**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae84b89",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n",
    "response = llm.invoke(\"Hello!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59e34c7",
   "metadata": {},
   "source": [
    "\n",
    "**After (Ollama):**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21cd9045",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(69486) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-ollama\n",
      "  Downloading langchain_ollama-1.0.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /Users/ac/Desktop/Github/Agents_in_Production/env/lib/python3.12/site-packages (from langchain-ollama) (1.2.2)\n",
      "Collecting ollama<1.0.0,>=0.6.0 (from langchain-ollama)\n",
      "  Downloading ollama-0.6.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /Users/ac/Desktop/Github/Agents_in_Production/env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /Users/ac/Desktop/Github/Agents_in_Production/env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.4.42)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /Users/ac/Desktop/Github/Agents_in_Production/env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (25.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/ac/Desktop/Github/Agents_in_Production/env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2.11.10)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /Users/ac/Desktop/Github/Agents_in_Production/env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/ac/Desktop/Github/Agents_in_Production/env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /Users/ac/Desktop/Github/Agents_in_Production/env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /Users/ac/Desktop/Github/Agents_in_Production/env/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.12.0)\n",
      "Requirement already satisfied: httpx>=0.27 in /Users/ac/Desktop/Github/Agents_in_Production/env/lib/python3.12/site-packages (from ollama<1.0.0,>=0.6.0->langchain-ollama) (0.28.1)\n",
      "Requirement already satisfied: anyio in /Users/ac/Desktop/Github/Agents_in_Production/env/lib/python3.12/site-packages (from httpx>=0.27->ollama<1.0.0,>=0.6.0->langchain-ollama) (4.11.0)\n",
      "Requirement already satisfied: certifi in /Users/ac/Desktop/Github/Agents_in_Production/env/lib/python3.12/site-packages (from httpx>=0.27->ollama<1.0.0,>=0.6.0->langchain-ollama) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/ac/Desktop/Github/Agents_in_Production/env/lib/python3.12/site-packages (from httpx>=0.27->ollama<1.0.0,>=0.6.0->langchain-ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/ac/Desktop/Github/Agents_in_Production/env/lib/python3.12/site-packages (from httpx>=0.27->ollama<1.0.0,>=0.6.0->langchain-ollama) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/ac/Desktop/Github/Agents_in_Production/env/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27->ollama<1.0.0,>=0.6.0->langchain-ollama) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/ac/Desktop/Github/Agents_in_Production/env/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (3.0.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /Users/ac/Desktop/Github/Agents_in_Production/env/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/ac/Desktop/Github/Agents_in_Production/env/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /Users/ac/Desktop/Github/Agents_in_Production/env/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /Users/ac/Desktop/Github/Agents_in_Production/env/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/ac/Desktop/Github/Agents_in_Production/env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/ac/Desktop/Github/Agents_in_Production/env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/ac/Desktop/Github/Agents_in_Production/env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/ac/Desktop/Github/Agents_in_Production/env/lib/python3.12/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ac/Desktop/Github/Agents_in_Production/env/lib/python3.12/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/ac/Desktop/Github/Agents_in_Production/env/lib/python3.12/site-packages (from anyio->httpx>=0.27->ollama<1.0.0,>=0.6.0->langchain-ollama) (1.3.1)\n",
      "Downloading langchain_ollama-1.0.1-py3-none-any.whl (29 kB)\n",
      "Downloading ollama-0.6.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: ollama, langchain-ollama\n",
      "Successfully installed langchain-ollama-1.0.1 ollama-0.6.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9741069",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How are you doing today? Is there something I can help you with or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1:8b\")\n",
    "response = llm.invoke(\"Hello!\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b3e4c9",
   "metadata": {},
   "source": [
    "\n",
    "# 5. Ollama API Parameters\n",
    "\n",
    "Ollama provides extensive customization options. Here are the most commonly used parameters:\n",
    "\n",
    "### Essential Parameters\n",
    "\n",
    "| **Parameter**    | **Type** | **Default** | **Description**                                                                              | **When to Use & How**                                                                                                                                                 |\n",
    "| ---------------- | -------- | ----------- | -------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `model`          | string   | ‚Äì           | **Required.** Model identifier (e.g., `\"llama3.1:8b\"`).                                      | Always required. Choose the model based on your task (e.g., use `llama3.1:8b` for fast and light response or `llama3.1:70b` for higher quality if resources allow). |\n",
    "| `messages`       | array    | ‚Äì           | Chat history as a list of `{role: user/assistant/system, content: ...}` messages.            | Required for context. |\n",
    "| `stream`         | boolean  | `true`      | Whether to stream the response token-by-token.                                               | Use `true` for real-time UI or chat apps. Use `false` if you need to process the whole response at once.|\n",
    "| `temperature`    | float    | `0.8`       | Controls randomness. `0.0` = deterministic, `2.0` = very random.                             |Lower (`0.0‚Äì0.3`) to reduce variability and make outputs more predictable. Increase (`>1.0`) when diverse or exploratory outputs are acceptable.|\n",
    "| `top_p`          | float    | `0.9`       | Nucleus sampling: only sample from top tokens whose cumulative probability is `top_p`.       | Lower (`0.5‚Äì0.8`) for conservative answers; keep `0.9` for balanced output. Use with or instead of `temperature`.                                                     |\n",
    "| `num_predict`    | int      | `128`       | Maximum number of tokens to generate. `-1` means unlimited (until `stop` or internal limit). | Increase for longer answers. Use smaller values (`50‚Äì150`) for concise answers, larger (`256+`) for essays. Limit to control latency.                                 |\n",
    "| `repeat_penalty` | float    | `1.1`       | Penalizes repetition. `1.0` = no penalty, >1 discourages repeated phrases.                   | Increase to `1.2‚Äì1.5` if the model repeats content. Keep at `1.0‚Äì1.1` for natural repetition (like poetry).                                                           |\n",
    "| `system`         | string   | ‚Äì           | Optional system prompt to set the behavior of the assistant.                                 | Use to define tone, expertise, or task. Example: `\"You are a helpful medical assistant.\"`                                                                             |\n",
    "| `stop`           | array    | ‚Äì           | List of strings that will stop generation when encountered.                                  | Use to prevent run-on responses. Example: `[\"\\nUser:\", \"</s>\"]` or custom delimiters for tools.                                                                       |\n",
    "\n",
    "\n",
    "### Performance Parameters\n",
    "\n",
    "| Parameter | Type | Default | Description |\n",
    "|-----------|------|---------|-------------|\n",
    "| `num_ctx` | int | 2048 | Context window size (max tokens in memory) |\n",
    "| `num_gpu` | int | -1 | GPU layers (-1=auto, 0=CPU only) |\n",
    "| `keep_alive` | string | - | Keep model loaded (\"5m\", \"10s\", \"-1\"=forever) |\n",
    "\n",
    "### Usage Examples\n",
    "\n",
    "**Basic API call with common parameters:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dabca6b1",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum computing! A fascinating and complex topic that has been gaining significant attention in recent years. I'll try to break it down in a way that's easy to understand.\n",
      "\n",
      "**What is Quantum Computing?**\n",
      "\n",
      "Classical computers use \"bits\" to process information, which can only be in one of two states: 0 or 1. Quantum computers, on the other hand, use \"qubits\" (quantum bits) that can exist in multiple states simultaneously. This property allows qubits to process a vast number of possibilities at the same time, making them incredibly powerful for certain types of calculations.\n",
      "\n",
      "**The Basics of Qubits**\n",
      "\n",
      "Qubits are special because they can:\n",
      "\n",
      "1. **Exist in multiple states**: A qubit can be both 0 and 1 at the same time, unlike classical bits.\n",
      "2. **Superposition**: This means that a qubit can represent all possible combinations of 0s and 1s simultaneously.\n",
      "3. **Entanglement**: When two or more qubits are connected, their properties become linked in a way that's not easily explained by classical physics.\n",
      "\n",
      "**How Quantum Computing Works**\n",
      "\n",
      "A quantum computer consists of several key components:\n",
      "\n",
      "1. **Qubit Register**: A collection of qubits that store the information to be processed.\n",
      "2. **Quantum Gate Operations**: These are the \"instructions\" applied to the qubits to perform calculations, such as rotations and entanglement.\n",
      "3. **Measurement**: This is where the quantum computer outputs its results, collapsing the superposition of states into a single measurement.\n",
      "\n",
      "**Quantum Computing Advantages**\n",
      "\n",
      "1. **Speedup**: Quantum computers can solve certain problems much faster than classical computers, making them ideal for simulations, cryptography, and optimization tasks.\n",
      "2. **Scalability**: As the number of qubits increases, so does the processing power, allowing quantum computers to tackle more complex problems.\n",
      "3. **Parallelism**: Qubits can process multiple possibilities simultaneously, reducing the need for sequential computation.\n",
      "\n",
      "**Challenges and Limitations**\n",
      "\n",
      "1. **Quantum Noise**: Qubits are prone to errors due to interactions with their environment, which can cause them to lose coherence (a property essential for quantum computing).\n",
      "2. **Scalability Challenges**: Currently, it's difficult to maintain control over large numbers of qubits.\n",
      "3. **Quantum Error Correction**: Developing methods to correct errors and ensure reliable operation is an ongoing challenge.\n",
      "\n",
      "**Real-World Applications**\n",
      "\n",
      "1. **Cryptography**: Quantum computers can potentially break certain encryption algorithms, but they can also be used for unbreakable quantum cryptography.\n",
      "2. **Optimization**: Quantum computing can help optimize complex systems, such as logistics, finance, and supply chain management.\n",
      "3. **Materials Science**: Quantum computers can simulate the behavior of materials at the atomic level, leading to breakthroughs in fields like chemistry and physics.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "Quantum computing is a rapidly evolving field that has the potential to revolutionize many areas of science, technology, and society. While it's still in its early stages, researchers are making significant progress in overcoming the challenges and limitations associated with quantum computing. As the technology continues to advance, we can expect to see exciting breakthroughs and applications emerge.\n",
      "\n",
      "**Further Reading**\n",
      "\n",
      "* \"Quantum Computation and Quantum Information\" by Michael A. Nielsen and Isaac L. Chuang (a comprehensive textbook)\n",
      "* \"The Quantum Universe\" by Brian Cox and Jeff Forshaw (an accessible introduction to quantum mechanics)\n",
      "* The IBM Quantum Experience (a cloud-based platform for exploring quantum computing)\n",
      "\n",
      "I hope this helps you understand the basics of quantum computing! Do you have any specific questions or would you like me to elaborate on a particular aspect?\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post(\"http://localhost:11434/api/chat\", json={\n",
    "\t\"model\": \"llama3.1:8b\",\n",
    "\t\"messages\": [{\"role\": \"user\", \"content\": \"Explain quantum computing\"}],\n",
    "\t\"stream\": False,\n",
    "\t\"temperature\": 0.3,  # Lower for factual responses\n",
    "\t\"top_p\": 0.9,  # Nucleus sampling\n",
    "\t\"num_predict\": 500,  # Limit response length\n",
    "\t\"repeat_penalty\": 1.2,  # Reduce repetition\n",
    "\t\"stop\": [\"```\", \"---\"]  # Stop at code blocks or separators\n",
    "})\n",
    "data = response.json()\n",
    "\n",
    "print(data[\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fc3620",
   "metadata": {},
   "source": [
    "\n",
    "**Using with LangChain:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dd52f66",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's an overview of the Toyota Land Cruiser generations:\n",
      "\n",
      "**1. First Generation (1960-1984) - FJ40**\n",
      "\n",
      "* Body-on-frame design\n",
      "* 2F or 3F engine, 3-speed automatic transmission\n",
      "* Rear-wheel drive with optional four-wheel drive system\n",
      "* Known for its simplicity and ruggedness\n",
      "\n",
      "**2. Second Generation (1984-1998) - FJ60/70**\n",
      "\n",
      "* Update of the FJ40 with a new body style and more modern features\n",
      "* 22R or 3F engine, 4-speed automatic transmission\n",
      "* Improved four-wheel drive system and suspension\n",
      "* Notable for its wider track and increased ground clearance\n",
      "\n",
      "**3. Third Generation (1998-2007) - 100**\n",
      "\n",
      "* Major redesign with a unibody construction and independent front suspension\n",
      "* 2FZ-FE or 1FZ-FE engine, 4-speed automatic transmission\n",
      "* Improved interior amenities and safety features\n",
      "* Known for its more refined ride and increased towing capacity\n",
      "\n",
      "**4. Fourth Generation (2007-2015) - 200**\n",
      "\n",
      "* New body style with a more angular design and improved aerodynamics\n",
      "* 3UR-FE or 2UR-F\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "\tmodel=\"llama3.1:8b\",\n",
    "\ttemperature=0.7,\n",
    "\ttop_p=0.9,\n",
    "\tnum_predict=256,\n",
    "\trepeat_penalty=1.1\n",
    ")\n",
    "response = llm.invoke(\"\"\"\n",
    "                      Can you give me a brief overview of all the Toyota Land Cruiser generations?\n",
    "                      Include their key differences and notable features.\n",
    "                      \"\"\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605c53b9",
   "metadata": {},
   "source": [
    "\n",
    "> **üí° Tips:** Use `stream: false` for simpler processing ‚Ä¢ Set `num_predict` to limit response length ‚Ä¢ Use `keep_alive` to avoid reloading models ‚Ä¢ Adjust `temperature` for creativity vs consistency\n",
    "\n",
    "> **‚ö†Ô∏è Note:** Not all parameters work with every model. Large `num_ctx` values require more RAM/VRAM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f27eda-48f5-4be5-b3b8-69073a5e52ed",
   "metadata": {},
   "source": [
    "# 6. Troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8d681c",
   "metadata": {},
   "source": [
    "**Model not found?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef683750-5fbb-4544-9f37-83d3037627bd",
   "metadata": {},
   "source": [
    "ollama pull <model-name>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a147a96",
   "metadata": {},
   "source": [
    "\n",
    "**Connection refused?**\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cedcd802-bbc4-4161-bb2b-e20a7f21559b",
   "metadata": {
    "language": "bash"
   },
   "source": [
    "ollama serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb51af1e-6d28-41c5-b42b-0f3691f220ed",
   "metadata": {},
   "source": [
    "**Out of memory?**\n",
    "Try a smaller model like `mistral:7b`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2323180-d70f-49d8-a874-6b1c647a90f5",
   "metadata": {},
   "source": [
    "# 7. Model Recommendations\n",
    "\n",
    "| Model | Size | Best For | Speed |\n",
    "|-------|------|----------|-------|\n",
    "| `llama3.1:8b` | 8GB RAM | General use, agents | Fast |\n",
    "| `qwen2.5:14b` | 14GB RAM | Code, reasoning | Medium |\n",
    "| `phi3:14b` | 14GB RAM | Efficient tasks | Fast |\n",
    "| `mistral:7b` | 7GB RAM | Simple tasks | Very Fast |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
